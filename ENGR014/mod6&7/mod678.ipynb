{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"ir","display_name":"R"},"language_info":{"name":"R"}},"cells":[{"cell_type":"markdown","source":["### **Topics Covered**\n","\n","#### **6.0 - 6.5: Foundations of Statistical Inference and Modeling**\n","\n","* ##### **6.0 Sampling Distributions and Confidence Intervals**\n","  Understanding variability in estimates and constructing confidence intervals.\n","\n","* ##### **6.1 Hypothesis Testing**\n","  Fundamentals of null and alternative hypotheses, test statistics, and p-values.\n","\n","* ##### **6.2 Analysis of Variance (Conceptual Introduction)**\n","  Principles of comparing group means and the logic behind the F-test.\n","\n","* ##### **6.3 Introduction to Simple Linear Regression**\n","  Estimating relationships between two continuous variables.\n","\n","* ##### **6.4 Introduction to Multiple Linear Regression**\n","  Extending linear models to include multiple predictors.\n","\n","* ##### **6.5 Model Diagnostics and Selection Strategies**\n","  Evaluating regression assumptions and selecting the best model.\n","\n","---\n","\n","#### **6.6 - 7.3: Applied Modeling and Complex ANOVA Designs**\n","\n","* ##### **6.6 Simple Linear Regression with Prediction**\n","  Building regression models for prediction, confidence/prediction intervals, and evaluating model reliability.\n","\n","* ##### **6.7 Multiple Linear Regression with Real-World Variables**\n","  Modeling multivariable relationships with interpretations and diagnostic checks.\n","\n","* ##### **6.8 Modeling Nonlinear Relationships with Regression**\n","  Using polynomial and nonlinear least squares models to capture curved data trends.\n","\n","* ##### **6.9 Binary Outcomes with Logistic Regression**\n","  Predicting categorical outcomes using the logit model and interpreting odds.\n","\n","* ##### **7.0 One-Way ANOVA for Multi-Group Comparisons**\n","  Testing whether means differ across multiple groups using a single factor.\n","\n","* ##### **7.1 One-Way ANOVA - In-Depth Application**\n","  Performing post-hoc tests, checking assumptions, and reporting ANOVA results comprehensively.\n","\n","* ##### **7.2 Two-Way ANOVA with Interaction Effects**\n","  Exploring how two categorical variables affect a continuous outcome, including interaction terms.\n","\n","* ##### **7.3 Mixed Predictors: Categorical + Quantitative (FEV Analysis)**\n","  Applying linear modeling to real-world data with both categorical and numeric variables (e.g., smoking, age, height).\n","* #### **7.4 Principal Component Analysis (PCA)**\n","  Reducing Complexity in Multivariable Datasets\n","\n","---"],"metadata":{"id":"r8nGoxm79IYl"}},{"cell_type":"markdown","source":["# **6.0 Sampling Distributions and Confidence Intervals**\n","\n","## Introduction\n","\n","In statistical inference, we often rely on **sample data** to make conclusions about an entire **population**. Because we usually cannot collect data from every member of a population, we take **samples**. But every sample is different, and so are the statistics calculated from them (like the mean or standard deviation).\n","\n","This is where the concept of **sampling distribution** and **confidence intervals** becomes crucial.\n","\n","---\n","\n","## **What is a Sampling Distribution?**\n","\n","A **sampling distribution** is the probability distribution of a given statistic (like the mean or median) based on a random sample.\n","\n","* It tells us **how much** a statistic (like the mean) would vary from one sample to another.\n","* The shape of the sampling distribution of the mean tends to be **normal** if the sample size is large enough, due to the **Central Limit Theorem (CLT)**.\n","\n","### Central Limit Theorem (CLT)\n","\n","> The Central Limit Theorem states that the **sampling distribution of the sample mean** will approximate a normal distribution as the sample size becomes large, regardless of the shape of the population distribution.\n","\n","This theorem is fundamental in statistics because it allows us to make probabilistic statements about where the true population parameter might lie.\n","\n","---\n","\n","## **Code Example: Simulating a Sampling Distribution**\n","\n","We will now simulate a sampling distribution of the sample mean."],"metadata":{"id":"b3vyNn4o_abL"}},{"cell_type":"code","source":["set.seed(123)\n","\n","# Step 1: Create a population of 10,000 values from a normal distribution\n","population <- rnorm(10000, mean = 100, sd = 15)\n","\n","# Step 2: Take 1000 samples (each of size 50) and calculate their means\n","sample_means <- replicate(1000, mean(sample(population, size = 50)))\n","\n","# Step 3: Plot histogram of sample means to visualize the sampling distribution\n","hist(sample_means, breaks = 30, col = \"skyblue\",\n","     main = \"Sampling Distribution of the Mean\", xlab = \"Sample Means\")"],"metadata":{"id":"6RU8zs7X_bpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`set.seed(123)`**:\n","\n","   * Ensures reproducibility. Using the same seed gives the same random numbers each time.\n","\n","2. **`rnorm(10000, mean = 100, sd = 15)`**:\n","\n","   * Generates 10,000 values from a **normal distribution** with a mean of 100 and standard deviation of 15. This simulates our **population**.\n","\n","3. **`replicate(1000, mean(sample(...)))`**:\n","\n","   * Repeats the sampling process 1000 times.\n","   * Each time, it draws a **random sample of 50 values** from the population.\n","   * It calculates the **mean** of each sample.\n","\n","4. **`hist(...)`**:\n","\n","   * Plots a histogram of the 1000 sample means, showing the **sampling distribution**.\n","\n","You should see a roughly **normal distribution** centered around the population mean of 100, even if individual samples vary.\n","\n","---\n","\n","## **Confidence Intervals (CI)**\n","\n","A **confidence interval** gives a range of values for estimating a population parameter (like the mean), along with a confidence level.\n","\n","> A **95% confidence interval** means that if we took 100 different samples and built a CI from each, approximately 95 of those intervals would contain the **true population mean**.\n","\n","### Formula for Confidence Interval of the Mean (when population standard deviation is unknown):\n","\n","$$\n","\\text{CI} = \\bar{x} \\pm t^* \\times \\frac{s}{\\sqrt{n}}\n","$$\n","\n","Where:\n","\n","* $\\bar{x}$ = sample mean\n","* $s$ = sample standard deviation\n","* $n$ = sample size\n","* $t^*$ = critical value from the t-distribution with $n-1$ degrees of freedom\n","\n","---\n","\n","## **Code Example: Constructing a 95% Confidence Interval**"],"metadata":{"id":"oy2-Fgxh_4-v"}},{"cell_type":"code","source":["# Take a random sample of 100 values from the population\n","sample_data <- sample(population, 100)\n","\n","# Calculate the sample mean\n","mean_sample <- mean(sample_data)\n","\n","# Calculate the standard error (standard deviation divided by sqrt of n)\n","se <- sd(sample_data) / sqrt(length(sample_data))\n","\n","# Use critical value for 95% confidence (z ≈ 1.96 for large samples)\n","ci_lower <- mean_sample - 1.96 * se\n","ci_upper <- mean_sample + 1.96 * se\n","\n","# Output the confidence interval\n","cat(\"95% Confidence Interval: [\", round(ci_lower, 2), \",\", round(ci_upper, 2), \"]\\n\")"],"metadata":{"id":"Q0xbvNK1_5sl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`sample(population, 100)`**:\n","\n","   * Randomly selects 100 values from the population.\n","\n","2. **`mean(sample_data)`**:\n","\n","   * Calculates the mean of the sample.\n","\n","3. **`sd(sample_data) / sqrt(length(sample_data))`**:\n","\n","   * Computes the **standard error (SE)**, which estimates how much the sample mean varies from the population mean.\n","\n","4. **`1.96 * se`**:\n","\n","   * For a 95% confidence level, we use **1.96** as the critical value from the normal distribution (used here since sample size is relatively large).\n","\n","5. **`cat(...)`**:\n","\n","   * Displays the confidence interval in a readable format.\n","\n","---\n","\n","## Summary of Key Terms\n","\n","| Term                  | Definition                                                    |\n","| --------------------- | ------------------------------------------------------------- |\n","| Sampling Distribution | Distribution of a sample statistic over many samples          |\n","| Central Limit Theorem | Sampling distribution of the mean becomes normal with large n |\n","| Standard Error        | Estimate of variability of a sample statistic                 |\n","| Confidence Interval   | A range in which the true population parameter likely lies    |\n","| 95% CI                | There's a 95% chance the interval captures the true mean      |\n","\n","---"],"metadata":{"id":"PNrwFu7YAaqL"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"D73gK5fGAvAV"}},{"cell_type":"markdown","source":["# **6.1 Hypothesis Testing**\n","\n","## Introduction\n","\n","**Hypothesis testing** is a formal statistical method used to make decisions or inferences about population parameters based on sample data.\n","\n","At the core of hypothesis testing is a **question**:\n","\n","> *Is the observed effect in my data statistically significant, or could it have occurred just by chance?*\n","\n","We frame this question by creating two competing statements:\n","\n","---\n","\n","## Null and Alternative Hypotheses\n","\n","* **Null Hypothesis (H₀)**:\n","  Assumes no effect, no difference, or no relationship.\n","  It's the \"status quo\" or \"default position\".\n","\n","  > Example: “The average height of students is 170 cm.” (H₀: μ = 170)\n","\n","* **Alternative Hypothesis (H₁ or Ha)**:\n","  Represents the claim or effect we're testing.\n","\n","  > Example: “The average height of students is not 170 cm.” (H₁: μ ≠ 170)\n","\n","---\n","\n","## Test Statistics and p-values\n","\n","After setting up hypotheses, we:\n","\n","1. Collect a **sample**.\n","2. Calculate a **test statistic** (e.g., t-value, z-score).\n","3. Determine the **p-value** (probability of observing the data assuming H₀ is true).\n","4. Compare the p-value to a predefined **significance level** (α, usually 0.05).\n","\n","### Decision Rule:\n","\n","* If **p-value ≤ α**, reject the null hypothesis.\n","* If **p-value > α**, do not reject the null hypothesis.\n","\n","---\n","\n","## Types of Hypothesis Tests\n","\n","| Type              | When to Use                   |\n","| ----------------- | ----------------------------- |\n","| One-sample t-test | Testing mean of one group     |\n","| Two-sample t-test | Comparing means of two groups |\n","| Paired t-test     | Before-and-after measurements |\n","| Proportion test   | Testing population proportion |\n","\n","We'll demonstrate a **One-sample t-test** using R.\n","\n","---\n","\n","## Code Example: One-Sample t-test\n","\n","Let's say we want to test whether the average test score of students is **different** from 75."],"metadata":{"id":"Hea67WmDCy_2"}},{"cell_type":"code","source":["set.seed(42)\n","\n","# Generate sample data (e.g., test scores of 30 students)\n","test_scores <- rnorm(30, mean = 73, sd = 5)\n","\n","# Perform one-sample t-test against a population mean of 75\n","result <- t.test(test_scores, mu = 75)\n","\n","# View the result\n","print(result)"],"metadata":{"id":"FeoCoH_YCzwA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`set.seed(42)`**:\n","   Sets a random seed to ensure reproducibility of the results.\n","\n","2. **`rnorm(30, mean = 73, sd = 5)`**:\n","   Simulates a sample of 30 test scores from a normal distribution with:\n","\n","   * Mean = 73\n","   * Standard deviation = 5\n","     This represents the **sample data**.\n","\n","3. **`t.test(test_scores, mu = 75)`**:\n","   Performs a **one-sample t-test**, testing whether the sample mean is significantly different from 75.\n","\n","4. **`print(result)`**:\n","   Displays:\n","\n","   * t-statistic\n","   * degrees of freedom (df)\n","   * p-value\n","   * confidence interval\n","   * sample mean\n","\n","---\n","\n","### Sample Output\n","\n","```\n","\tOne Sample t-test\n","\n","data:  test_scores\n","t = -2.1785, df = 29, p-value = 0.0378\n","alternative hypothesis: true mean is not equal to 75\n","95 percent confidence interval:\n"," 70.94 74.89\n","sample estimates:\n","mean of x\n","     72.91\n","```\n","\n","---\n","\n","## Interpreting the Results\n","\n","* **t = -2.18**:\n","  The test statistic. The negative sign means the sample mean is less than 75.\n","\n","* **p-value = 0.0378**:\n","  Since this is less than α = 0.05, we **reject the null hypothesis**.\n","  → There is evidence that the average score is **significantly different** from 75.\n","\n","* **95% Confidence Interval: \\[70.94, 74.89]**:\n","  This range does **not** include 75, reinforcing the conclusion.\n","\n","---\n","\n","## Assumptions of the t-test\n","\n","* The data are **normally distributed** (especially important with small samples).\n","* Observations are **independent**.\n","* The population **standard deviation is unknown** (if it’s known, we use a z-test).\n","\n","---\n","\n","## Summary Table\n","\n","| Component                   | Description                                          |\n","| --------------------------- | ---------------------------------------------------- |\n","| Null Hypothesis (H₀)        | Assumes no difference                                |\n","| Alternative Hypothesis (H₁) | Assumes a difference exists                          |\n","| Test Statistic              | Measures how far sample mean is from population mean |\n","| p-value                     | Probability of observing the data if H₀ is true      |\n","| α (alpha)                   | Significance level (commonly 0.05)                   |\n","| Decision Rule               | Reject H₀ if p ≤ α                                   |\n","\n","---"],"metadata":{"id":"LKD-gJ1MDUGa"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"IA1UEZDGDvsu"}},{"cell_type":"markdown","source":["# **6.2 Analysis of Variance (ANOVA)**\n","\n","## Introduction\n","\n","**Analysis of Variance (ANOVA)** is a statistical technique used to determine whether there are **significant differences between the means** of three or more independent groups.\n","\n","While a **t-test** can compare the means of two groups, **ANOVA** allows us to test **multiple groups simultaneously**.\n","\n","---\n","\n","## Purpose of ANOVA\n","\n","Suppose you want to compare the average test scores of students from **three different schools**. Running multiple t-tests would increase the risk of **Type I error** (false positive). ANOVA helps to:\n","\n","* Test whether **at least one** group mean differs from the others.\n","* Maintain the overall error rate.\n","\n","---\n","\n","## Hypotheses in ANOVA\n","\n","Let's say you have three groups: A, B, and C.\n","\n","* **Null Hypothesis (H₀):**\n","  All group means are equal.\n","  $\\mu_A = \\mu_B = \\mu_C$\n","\n","* **Alternative Hypothesis (H₁):**\n","  At least one group mean is different.\n","\n","---\n","\n","## How ANOVA Works\n","\n","ANOVA partitions the total variability in the data into:\n","\n","1. **Between-group variance (SSB):**\n","   Variability due to differences between the group means.\n","\n","2. **Within-group variance (SSW):**\n","   Variability within each group (i.e., due to random error).\n","\n","It then calculates the **F-statistic**:\n","\n","$$\n","F = \\frac{\\text{Between-group variability (MSB)}}{\\text{Within-group variability (MSW)}}\n","$$\n","\n","* A **large F-value** indicates that the group means are likely different.\n","* The **p-value** tells us whether this difference is statistically significant.\n","\n","---\n","\n","## Assumptions of ANOVA\n","\n","1. **Independence:** Observations are independent of one another.\n","2. **Normality:** Data in each group is approximately normally distributed.\n","3. **Homoscedasticity:** Equal variances across groups.\n","\n","---\n","\n","## Code Example: One-Way ANOVA\n","\n","We'll simulate data for 3 different teaching methods (A, B, and C) and test whether their average student scores differ."],"metadata":{"id":"_tWd0tg8DyQG"}},{"cell_type":"code","source":["set.seed(100)\n","\n","# Create group labels\n","group <- rep(c(\"A\", \"B\", \"C\"), each = 30)\n","\n","# Simulate scores for each group\n","scores <- c(\n","  rnorm(30, mean = 75, sd = 5),  # Group A\n","  rnorm(30, mean = 80, sd = 5),  # Group B\n","  rnorm(30, mean = 78, sd = 5)   # Group C\n",")\n","\n","# Create a data frame\n","data <- data.frame(Method = group, Score = scores)\n","\n","# Perform one-way ANOVA\n","anova_result <- aov(Score ~ Method, data = data)\n","\n","# Display ANOVA table\n","summary(anova_result)"],"metadata":{"id":"TzvF1lH1DzFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`set.seed(100)`**:\n","   Ensures that the randomly generated numbers are reproducible.\n","\n","2. **`rep(c(\"A\", \"B\", \"C\"), each = 30)`**:\n","   Creates a group variable with 30 students in each group (90 total).\n","\n","3. **`rnorm(..., mean, sd)`**:\n","   Generates 30 normally distributed scores for each group with a different mean and the same standard deviation.\n","\n","4. **`data.frame(...)`**:\n","   Combines the group labels and scores into a structured dataset.\n","\n","5. **`aov(Score ~ Method, data = data)`**:\n","   Fits an ANOVA model.\n","\n","   * `Score` is the response (dependent variable).\n","   * `Method` is the explanatory (independent categorical) variable.\n","\n","6. **`summary(anova_result)`**:\n","   Outputs the ANOVA table with columns:\n","\n","   * **Df**: Degrees of freedom\n","   * **Sum Sq**: Sum of Squares\n","   * **Mean Sq**: Mean Sum of Squares\n","   * **F value**: F-statistic\n","   * **Pr(>F)**: p-value\n","\n","---\n","\n","### Sample Output (May Vary Slightly)\n","\n","```\n","            Df Sum Sq Mean Sq F value Pr(>F)    \n","Method       2   527.6  263.80   11.83  1.3e-05 ***\n","Residuals   87  1940.2   22.30                    \n","```\n","\n","---\n","\n","## Interpreting the Results\n","\n","* **F value = 11.83**:\n","  Indicates the ratio of between-group to within-group variance. A higher value suggests stronger evidence against H₀.\n","\n","* **Pr(>F) = 1.3e-05** (i.e., 0.000013):\n","  Very small p-value. Since it is less than 0.05, we **reject the null hypothesis**.\n","  → At least one group has a significantly different mean.\n","\n","---\n","\n","## Post-Hoc Testing (Optional but Important)\n","\n","ANOVA only tells us **that a difference exists**, not **where** it lies. For that, we perform a **post-hoc test** such as **Tukey's Honest Significant Difference (HSD)**."],"metadata":{"id":"l5xco2KVEjFm"}},{"cell_type":"code","source":["# Tukey HSD test\n","tukey_result <- TukeyHSD(anova_result)\n","\n","# View pairwise comparisons\n","print(tukey_result)"],"metadata":{"id":"rwNXyUxnEj62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This tells us **which pairs of groups** are significantly different.\n","\n","---\n","\n","## Summary of Key Terms\n","\n","| Term                   | Definition                                                    |\n","| ---------------------- | ------------------------------------------------------------- |\n","| ANOVA                  | Tests for differences in means across multiple groups         |\n","| Between-group variance | Variability due to differences between group means            |\n","| Within-group variance  | Variability within each group                                 |\n","| F-statistic            | Ratio of between-group to within-group variance               |\n","| p-value                | Probability of observing the result under the null hypothesis |\n","| Tukey HSD              | Post-hoc test for pairwise group comparison                   |\n"],"metadata":{"id":"lGuWQORfDzz7"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"nIemW5_lE9EA"}},{"cell_type":"markdown","source":["# **6.3 Simple Linear Regression**\n","\n","## Introduction\n","\n","**Simple Linear Regression** is a method for modeling the **relationship between two quantitative variables**:\n","\n","* An **independent variable** (also called the predictor or explanatory variable)\n","* A **dependent variable** (also called the response or outcome variable)\n","\n","The goal is to model how the dependent variable changes when the independent variable changes.\n","\n","---\n","\n","## The Regression Model\n","\n","The general form of a simple linear regression model is:\n","\n","$$\n","Y = \\beta_0 + \\beta_1X + \\epsilon\n","$$\n","\n","Where:\n","\n","* $Y$ = dependent variable\n","* $X$ = independent variable\n","* $\\beta_0$ = intercept (value of Y when X = 0)\n","* $\\beta_1$ = slope (change in Y for a 1-unit change in X)\n","* $\\epsilon$ = error term (accounts for the variation not explained by the model)\n","\n","---\n","\n","## Assumptions of Linear Regression\n","\n","1. **Linearity**: The relationship between X and Y is linear.\n","2. **Independence**: Observations are independent.\n","3. **Homoscedasticity**: Constant variance of residuals.\n","4. **Normality**: Residuals are normally distributed.\n","\n","If these assumptions hold, linear regression provides **unbiased estimates** of the coefficients.\n","\n","---\n","\n","## Code Example: Simple Linear Regression in R\n","\n","Let's simulate a dataset and fit a simple linear regression model."],"metadata":{"id":"4EwiLFshE_Ro"}},{"cell_type":"code","source":["set.seed(101)\n","\n","# Generate synthetic data\n","x <- rnorm(100, mean = 10, sd = 2)           # Independent variable\n","y <- 3 + 1.5 * x + rnorm(100, mean = 0, sd = 2)  # Dependent variable with noise\n","\n","# Create a data frame\n","df <- data.frame(x, y)\n","\n","# Fit a simple linear regression model\n","model <- lm(y ~ x, data = df)\n","\n","# View the summary of the model\n","summary(model)"],"metadata":{"id":"DpIqlMvRFAoM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`set.seed(101)`**:\n","   Makes sure the random numbers generated are the same each time you run the code.\n","\n","2. **`x <- rnorm(100, mean = 10, sd = 2)`**:\n","   Generates 100 normally distributed values with a mean of 10 and standard deviation of 2. This is your **independent variable (X)**.\n","\n","3. **`y <- 3 + 1.5 * x + rnorm(100, mean = 0, sd = 2)`**:\n","   Creates the **dependent variable (Y)**.\n","   The true model here is $y = 3 + 1.5x + \\text{noise}$.\n","\n","4. **`df <- data.frame(x, y)`**:\n","   Combines the variables into a data frame.\n","\n","5. **`lm(y ~ x, data = df)`**:\n","   Fits a linear model predicting `y` from `x`.\n","\n","6. **`summary(model)`**:\n","   Provides estimates of coefficients, standard errors, R-squared, and p-values.\n","\n","---\n","\n","### Example Output from `summary(model)`\n","\n","```\n","Call:\n","lm(formula = y ~ x, data = df)\n","\n","Coefficients:\n","            Estimate Std. Error t value Pr(>|t|)    \n","(Intercept)   2.8710     0.5997   4.787 6.52e-06 ***\n","x             1.4885     0.0569  26.141  < 2e-16 ***\n","```\n","\n","---\n","\n","## Interpreting the Results\n","\n","* **Intercept = 2.8710**:\n","  When `x = 0`, the expected value of `y` is 2.87.\n","\n","* **Slope = 1.4885**:\n","  For every 1-unit increase in `x`, `y` increases by about 1.49 units.\n","\n","* **p-values**:\n","\n","  * Both the intercept and the slope are statistically significant (**p < 0.05**).\n","\n","* **R-squared (in output)**:\n","  This tells us how much of the variance in `y` is explained by `x`.\n","  An R² of 0.88 means **88% of the variation in y** is explained by the model.\n","\n","---\n","\n","## Visualizing the Regression Line"],"metadata":{"id":"nsZFabSDFqPj"}},{"cell_type":"code","source":["# Scatter plot with regression line\n","plot(df$x, df$y, main = \"Simple Linear Regression\",\n","     xlab = \"X\", ylab = \"Y\", pch = 19, col = \"darkgray\")\n","abline(model, col = \"blue\", lwd = 2)"],"metadata":{"id":"6u5GBNLKFq8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation\n","\n","* **`plot(...)`**: Creates a scatter plot of `x` and `y`.\n","* **`abline(model)`**: Adds the regression line from the fitted model.\n","\n","The line represents the predicted values of `y` for each `x`.\n","\n","---\n","\n","## Model Diagnostics\n","\n","You can evaluate how well the model fits and check for assumption violations using diagnostic plots:"],"metadata":{"id":"_fE0K1NgGBPc"}},{"cell_type":"code","source":["par(mfrow = c(2, 2))  # Layout for 4 plots\n","plot(model)"],"metadata":{"id":"C03AkacAGCFr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The four default plots are:\n","\n","1. **Residuals vs Fitted** - checks linearity and homoscedasticity.\n","2. **Normal Q-Q** - checks if residuals are normally distributed.\n","3. **Scale-Location** - checks spread of residuals.\n","4. **Residuals vs Leverage** - identifies influential observations.\n","\n","---\n","\n","## Summary Table\n","\n","| Term           | Meaning                                           |\n","| -------------- | ------------------------------------------------- |\n","| Intercept (β₀) | Expected value of Y when X = 0                    |\n","| Slope (β₁)     | Change in Y for a one-unit change in X            |\n","| R-squared      | Proportion of variance in Y explained by X        |\n","| Residuals      | Differences between observed and predicted values |\n","| p-value        | Indicates significance of predictor               |\n"],"metadata":{"id":"RhowOfAeFBR_"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"DEUvEUfsGZJC"}},{"cell_type":"markdown","source":["# **6.4 Multiple Linear Regression**\n","\n","## Introduction\n","\n","While **simple linear regression** models the relationship between **one predictor** and a **response variable**, **multiple linear regression** allows you to model the relationship between **two or more predictors** and a response.\n","\n","This provides a more flexible and realistic model, especially in complex real-world problems where outcomes are influenced by several factors.\n","\n","---\n","\n","## The Multiple Linear Regression Model\n","\n","The general form:\n","\n","$$\n","Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_kX_k + \\epsilon\n","$$\n","\n","Where:\n","\n","* $Y$ = response variable\n","* $X_1, X_2, \\ldots, X_k$ = predictor (independent) variables\n","* $\\beta_0$ = intercept\n","* $\\beta_1, \\beta_2, \\ldots \\beta_k$ = regression coefficients\n","* $\\epsilon$ = error term\n","\n","Each coefficient $\\beta_i$ represents the **expected change in Y** for a **one-unit increase in $X_i$**, **holding all other predictors constant**.\n","\n","---\n","\n","## Assumptions of Multiple Linear Regression\n","\n","1. **Linearity**: Linear relationship between predictors and response.\n","2. **Independence**: Observations are independent.\n","3. **Homoscedasticity**: Constant variance of residuals.\n","4. **Normality**: Residuals are normally distributed.\n","5. **No multicollinearity**: Predictors are not highly correlated with each other.\n","\n","---\n","\n","## Code Example: Fitting a Multiple Linear Regression in R\n","\n","Let's simulate a dataset with two predictors (`x1`, `x2`) and one outcome variable (`y`)."],"metadata":{"id":"fRuR6crrGbBn"}},{"cell_type":"code","source":["set.seed(202)\n","\n","# Create predictors\n","x1 <- rnorm(100, mean = 50, sd = 10)      # Predictor 1\n","x2 <- rnorm(100, mean = 30, sd = 5)       # Predictor 2\n","\n","# Create dependent variable\n","y <- 10 + 0.5 * x1 + 1.2 * x2 + rnorm(100, sd = 4)\n","\n","# Combine into a data frame\n","data <- data.frame(y, x1, x2)\n","\n","# Fit multiple linear regression model\n","model <- lm(y ~ x1 + x2, data = data)\n","\n","# Display summary\n","summary(model)"],"metadata":{"id":"aA9VNlfnGb5b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","1. **`set.seed(202)`**: Ensures reproducibility.\n","\n","2. **`x1` and `x2`**: Generated as independent variables from normal distributions.\n","\n","3. **`y`**: Created based on a linear model $y = 10 + 0.5x_1 + 1.2x_2$ with some random noise.\n","\n","4. **`lm(y ~ x1 + x2)`**:\n","   Fits a multiple linear regression model with `x1` and `x2` as predictors.\n","\n","5. **`summary(model)`**:\n","   Provides model coefficients, R-squared, and significance tests.\n","\n","---\n","\n","### Sample Output (May Vary)\n","\n","```\n","Call:\n","lm(formula = y ~ x1 + x2, data = data)\n","\n","Coefficients:\n","            Estimate Std. Error t value Pr(>|t|)    \n","(Intercept)  10.0594     1.4157   7.102 2.34e-10 ***\n","x1            0.4839     0.0285  17.003  < 2e-16 ***\n","x2            1.1764     0.0592  19.873  < 2e-16 ***\n","```\n","\n","---\n","\n","## Interpreting the Output\n","\n","* **Intercept (10.0594)**:\n","  The expected value of `y` when both `x1` and `x2` are 0.\n","\n","* **x1 Coefficient (0.4839)**:\n","  When `x1` increases by 1 unit, `y` increases by **0.48 units**, holding `x2` constant.\n","\n","* **x2 Coefficient (1.1764)**:\n","  When `x2` increases by 1 unit, `y` increases by **1.18 units**, holding `x1` constant.\n","\n","* **Significance (p-values)**:\n","  All predictors are statistically significant (p < 0.05).\n","\n","* **R-squared (from summary)**:\n","  Represents how much of the variation in `y` is explained by the model. An R² near 1 is ideal.\n","\n","---\n","\n","## Checking for Multicollinearity\n","\n","**Multicollinearity** occurs when predictor variables are highly correlated. This can distort the estimates of coefficients.\n","\n","To detect it, we use **Variance Inflation Factor (VIF)**:"],"metadata":{"id":"JFEXq8Q0HNOY"}},{"cell_type":"code","source":["# Install and load car package if not already\n","install.packages(\"car\")\n","library(car)\n","\n","vif(model)"],"metadata":{"id":"nCH-mIaeHN9H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* VIF values:\n","\n","  * **1 to 5**: acceptable\n","  * **>10**: severe multicollinearity\n","\n","---\n","\n","## Diagnostic Plots\n","\n","You can check for assumption violations using diagnostic plots:"],"metadata":{"id":"f98Gh5h9I9sx"}},{"cell_type":"code","source":["par(mfrow = c(2, 2))\n","plot(model)"],"metadata":{"id":"KtSETSevI-nQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will display:\n","\n","1. Residuals vs Fitted\n","2. Normal Q-Q Plot\n","3. Scale-Location Plot\n","4. Residuals vs Leverage\n","\n","Look for:\n","\n","* Linear spread in residuals vs fitted plot\n","* Straight line in Q-Q plot (normality)\n","* No funnel shape in Scale-Location (homoscedasticity)\n","\n","---\n","\n","## Making Predictions\n","\n","You can use the model to make predictions for new data:"],"metadata":{"id":"Tx-775YCJWdK"}},{"cell_type":"code","source":["new_data <- data.frame(x1 = c(60, 55), x2 = c(35, 28))\n","predict(model, newdata = new_data)"],"metadata":{"id":"_XOjjUS4JXBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Returns predicted `y` values based on the model.\n","\n","---\n","\n","## Summary Table\n","\n","| Term                | Meaning                                                |\n","| ------------------- | ------------------------------------------------------ |\n","| Multiple Regression | Model with multiple predictors                         |\n","| Coefficient (β)     | Expected change in Y from a 1-unit change in predictor |\n","| R-squared           | % of variation in Y explained by predictors            |\n","| VIF                 | Measures multicollinearity                             |\n","| Residuals           | Observed - Predicted values                            |\n"],"metadata":{"id":"CUe0HMdxGdJR"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"KivehSWjJl48"}},{"cell_type":"markdown","source":["# **6.5 Linear Model Selection and Diagnostics**\n","\n","## Introduction\n","\n","In real-world data, you often face the challenge of choosing the **best subset of predictors** for your regression model. Including **too many variables** can cause overfitting, while too few may leave out important information. At the same time, even a good model may violate regression assumptions — so we must **check diagnostics** to validate it.\n","\n","This section will cover:\n","\n","* Model selection techniques\n","* Assumption checks\n","* Influence and leverage\n","* Residual analysis\n","* Tools like AIC, BIC, VIF, and diagnostic plots\n","\n","---\n","\n","## 1. **Model Selection**\n","\n","Model selection is about choosing the combination of predictors that best explains the dependent variable without overfitting. Some methods:\n","\n","### A. **Forward Selection**\n","\n","Starts with no predictors, adds one at a time based on the most significant improvement in model fit.\n","\n","### B. **Backward Elimination**\n","\n","Starts with all predictors, removes the least significant one at each step.\n","\n","### C. **Stepwise Regression**\n","\n","A combination of forward and backward methods. R uses **Akaike Information Criterion (AIC)** to evaluate models.\n","\n","---\n","\n","### Code Example: Stepwise Model Selection"],"metadata":{"id":"NZQIlu2PJn_g"}},{"cell_type":"code","source":["# Assume a full model with many predictors\n","set.seed(123)\n","x1 <- rnorm(100)\n","x2 <- rnorm(100)\n","x3 <- rnorm(100)\n","y <- 5 + 2*x1 - 3*x2 + rnorm(100)\n","\n","df <- data.frame(y, x1, x2, x3)\n","\n","# Full model\n","full_model <- lm(y ~ x1 + x2 + x3, data = df)\n","\n","# Stepwise selection based on AIC\n","step_model <- step(full_model, direction = \"both\")\n","\n","# Summary of the selected model\n","summary(step_model)"],"metadata":{"id":"3OPVYLzmJooR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Explanation\n","\n","* **`step()`**: Performs stepwise regression by adding/removing variables to minimize AIC.\n","* **AIC (Akaike Information Criterion)**: A lower AIC indicates a better model (balances fit with simplicity).\n","\n","---\n","\n","## 2. **Multicollinearity Diagnostics**\n","\n","Multicollinearity occurs when predictors are highly correlated, leading to unstable coefficient estimates.\n","\n","### Use **Variance Inflation Factor (VIF)** to detect:"],"metadata":{"id":"sKq7JNqWKQgF"}},{"cell_type":"code","source":["library(car)\n","vif(step_model)"],"metadata":{"id":"vzTzaAtuKRXb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* VIF > 5 suggests moderate multicollinearity.\n","* VIF > 10 indicates serious multicollinearity.\n","\n","---\n","\n","## 3. **Model Assumption Checks (Diagnostics)**\n","\n","After fitting a regression model, we must ensure the following:\n","\n","| Check                 | Purpose                              |\n","| --------------------- | ------------------------------------ |\n","| Residuals vs Fitted   | Check linearity and homoscedasticity |\n","| Q-Q Plot              | Check normality of residuals         |\n","| Scale-Location        | Check spread (variance) of residuals |\n","| Residuals vs Leverage | Identify influential observations    |\n","\n","### Generate Diagnostic Plots in R:"],"metadata":{"id":"csOk0HIHKeRM"}},{"cell_type":"code","source":["par(mfrow = c(2, 2))  # Set up plotting grid\n","plot(step_model)"],"metadata":{"id":"af9rcoifKfTu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How to Interpret the Plots\n","\n","1. **Residuals vs Fitted**:\n","\n","   * Should show a random scatter (no pattern). A curve suggests non-linearity.\n","\n","2. **Normal Q-Q**:\n","\n","   * Points should follow the straight line. Deviations imply non-normal residuals.\n","\n","3. **Scale-Location (Spread vs Fitted)**:\n","\n","   * Points should be spread equally. A fan shape indicates heteroscedasticity.\n","\n","4. **Residuals vs Leverage**:\n","\n","   * Identifies **influential points**. Look for points with high leverage or large Cook's distance.\n","\n","---\n","\n","## 4. **Detecting Outliers and Influential Observations**\n","\n","Influential points can overly affect the model. One tool to detect them is **Cook's Distance**:"],"metadata":{"id":"bq0Ii6EPKvpr"}},{"cell_type":"code","source":["cooksd <- cooks.distance(step_model)\n","plot(cooksd, type = \"h\", main = \"Cook's Distance\", ylab = \"Influence\")\n","abline(h = 4/length(cooksd), col = \"red\")"],"metadata":{"id":"3xIb_vrXKwaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Observations above the red line may be **influential**.\n","* Use this to decide if you need to remove or investigate those data points.\n","\n","---\n","\n","## 5. **Comparing Models Using AIC and BIC**\n","\n","If you have multiple models, compare them using AIC and BIC:\n","\n","```r\n","AIC(model1, model2)\n","BIC(model1, model2)\n","```\n","\n","* Lower AIC/BIC values indicate a better model (but BIC penalizes model complexity more heavily than AIC).\n","\n","---\n","\n","## 6. **Model Validation**\n","\n","### Split your data into **training and testing** sets to evaluate performance:"],"metadata":{"id":"OgYpk96_K_u3"}},{"cell_type":"code","source":["set.seed(123)\n","index <- sample(1:nrow(df), size = 0.7*nrow(df))\n","train <- df[index, ]\n","test <- df[-index, ]\n","\n","# Fit model on training data\n","trained_model <- lm(y ~ x1 + x2, data = train)\n","\n","# Predict on test data\n","predicted <- predict(trained_model, newdata = test)\n","\n","# Calculate RMSE\n","rmse <- sqrt(mean((test$y - predicted)^2))\n","print(rmse)"],"metadata":{"id":"_A-MMtzkLAsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary of Tools and Concepts\n","\n","| Term                | Purpose                          |\n","| ------------------- | -------------------------------- |\n","| Stepwise Regression | Select best predictors using AIC |\n","| VIF                 | Detect multicollinearity         |\n","| AIC/BIC             | Model selection criteria         |\n","| Residual Plots      | Check assumptions                |\n","| Cook's Distance     | Identify influential data points |\n","| RMSE                | Evaluate predictive accuracy     |\n","\n","---\n","\n","## Conclusion\n","\n","This section wraps up your introduction to **statistical modeling** in R:\n","\n","* **Model selection** helps you balance complexity and performance.\n","* **Diagnostic tools** ensure your model is valid, interpretable, and reliable.\n","* Always **visualize, test assumptions, and validate** before using a model for prediction or decision-making.\n"],"metadata":{"id":"7AF522l6Jped"}},{"cell_type":"markdown","source":["### **Regression and Analysis of variance (ANOVA)**"],"metadata":{"id":"-BeFvx8TLveq"}},{"cell_type":"markdown","source":["# **6.6 Simple Linear Regression: Prediction**\n","\n","## Objective\n","\n","In this section, we extend simple linear regression beyond just modeling — we will use it to **make predictions** and **evaluate prediction uncertainty**.\n","\n","---\n","\n","## Recap of Simple Linear Regression Model\n","\n","The model:\n","\n","$$\n","Y = \\beta_0 + \\beta_1 X + \\epsilon\n","$$\n","\n","Once we fit a model to data, we can use it to predict the outcome (`Y`) for new values of the predictor (`X`).\n","\n","---\n","\n","## Two Types of Prediction\n","\n","1. **Point Prediction**\n","\n","   * Estimate of the mean response at a given value of X\n","   * Example: What is the expected income of someone with 10 years of education?\n","\n","2. **Prediction Interval**\n","\n","   * Range where a **new individual response** is likely to fall\n","   * Wider than the confidence interval for mean prediction because it includes error variance\n","\n","---\n","\n","## Code Example: Prediction in Simple Linear Regression\n","\n","Let's build a model and then use it to predict values for new data."],"metadata":{"id":"ohSLLqGhkD3j"}},{"cell_type":"code","source":["# Simulated data\n","set.seed(101)\n","x <- rnorm(100, mean = 10, sd = 2)\n","y <- 2 + 0.8 * x + rnorm(100, sd = 1.5)\n","df <- data.frame(x, y)\n","\n","# Fit the regression model\n","model <- lm(y ~ x, data = df)\n","\n","# Summary of the model\n","summary(model)\n","\n","# Predict y for a new value of x = 12\n","new_data <- data.frame(x = 12)\n","\n","# 1. Point estimate (mean response)\n","predict(model, newdata = new_data)\n","\n","# 2. Confidence interval (95%) for mean response\n","predict(model, newdata = new_data, interval = \"confidence\")\n","\n","# 3. Prediction interval (95%) for an individual response\n","predict(model, newdata = new_data, interval = \"prediction\")"],"metadata":{"id":"YazjFe-QkE7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Code\n","\n","* **`predict(..., interval = \"confidence\")`**:\n","  Returns the estimated **mean value** of `y` and its **confidence interval**.\n","\n","* **`predict(..., interval = \"prediction\")`**:\n","  Predicts where **an individual value** of `y` might fall. This range is **wider** because it includes both:\n","\n","  * Uncertainty in the regression line\n","  * Natural variability of data points around the line\n","\n","---\n","\n","### Sample Output\n","\n","```r\n","fit      lwr      upr\n","11.6    10.9     12.3   # confidence interval\n","11.6     8.2     15.0   # prediction interval\n","```\n","\n","* The **mean predicted value** at x = 12 is **11.6**\n","* A new observation at x = 12 would likely fall between **8.2 and 15.0**\n","\n","---\n","\n","## Visualizing Prediction vs Confidence Intervals"],"metadata":{"id":"okLsz72vkqAa"}},{"cell_type":"code","source":["# Plot with regression line\n","plot(df$x, df$y, main = \"Prediction & Confidence Intervals\", xlab = \"x\", ylab = \"y\")\n","abline(model, col = \"blue\")\n","\n","# Create prediction frame\n","x_vals <- seq(min(df$x), max(df$x), length.out = 100)\n","pred_frame <- data.frame(x = x_vals)\n","conf_int <- predict(model, newdata = pred_frame, interval = \"confidence\")\n","pred_int <- predict(model, newdata = pred_frame, interval = \"prediction\")\n","\n","# Add confidence interval (mean)\n","lines(x_vals, conf_int[,2], col = \"green\", lty = 2)  # lower bound\n","lines(x_vals, conf_int[,3], col = \"green\", lty = 2)  # upper bound\n","\n","# Add prediction interval (individual)\n","lines(x_vals, pred_int[,2], col = \"red\", lty = 3)  # lower bound\n","lines(x_vals, pred_int[,3], col = \"red\", lty = 3)  # upper bound\n","\n","legend(\"topleft\", legend = c(\"Regression Line\", \"Confidence Interval\", \"Prediction Interval\"),\n","       col = c(\"blue\", \"green\", \"red\"), lty = c(1, 2, 3), bty = \"n\")"],"metadata":{"id":"CDCcfa8Xkq7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Interpretation\n","\n","* **Blue line**: regression line (best fit)\n","* **Green dashed lines**: 95% confidence interval for the mean prediction\n","* **Red dotted lines**: 95% prediction interval for a new individual\n","\n","---\n","\n","## Summary of Key Concepts\n","\n","| Concept             | Meaning                                      |\n","| ------------------- | -------------------------------------------- |\n","| Point Prediction    | Single estimated value of Y                  |\n","| Confidence Interval | Range for the average Y at a specific X      |\n","| Prediction Interval | Range for an individual Y at a specific X    |\n","| `predict()`         | R function to make predictions and intervals |\n","\n","---"],"metadata":{"id":"eiBvAVGzhKlk"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"-mCmJIvck92B"}},{"cell_type":"markdown","source":["# **6.7 Multiple Linear Regression**\n","\n","## Objective\n","\n","**Multiple Linear Regression (MLR)** extends simple linear regression by allowing **two or more independent variables (predictors)** to explain variation in a **single dependent (response) variable**.\n","\n","The aim is to understand the **combined and individual effect** of predictors on the outcome.\n","\n","---\n","\n","## The Model\n","\n","The general form of a multiple linear regression model is:\n","\n","$$\n","Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k + \\varepsilon\n","$$\n","\n","Where:\n","\n","* $Y$: Dependent (response) variable\n","* $X_1, X_2, ..., X_k$: Independent (predictor) variables\n","* $\\beta_0$: Intercept (value of Y when all Xs are zero)\n","* $\\beta_1, ..., \\beta_k$: Coefficients (slopes for each predictor)\n","* $\\varepsilon$: Error term (unexplained variability)\n","\n","---\n","\n","## Example Scenario\n","\n","Let's assume you are modeling **student test scores** based on:\n","\n","* Hours studied (`study_hours`)\n","* Attendance rate (`attendance`)\n","* Sleep duration the night before (`sleep_hours`)\n","\n","You want to understand:\n","\n","* How each variable contributes to performance\n","* Whether all of them are necessary in your model\n","* How accurately you can predict scores using them\n","\n","---\n","\n","## Step-by-Step in R\n","\n","### Step 1: Simulate Data"],"metadata":{"id":"LzO3xHq-l1gw"}},{"cell_type":"code","source":["set.seed(2024)\n","\n","# Create predictor variables\n","study_hours <- rnorm(100, mean = 10, sd = 2)\n","attendance <- rnorm(100, mean = 90, sd = 5)\n","sleep_hours <- rnorm(100, mean = 7, sd = 1)\n","\n","# Create a response variable\n","score <- 50 + 2 * study_hours + 0.5 * attendance + 1.5 * sleep_hours + rnorm(100, sd = 5)\n","\n","# Combine into a data frame\n","df <- data.frame(score, study_hours, attendance, sleep_hours)"],"metadata":{"id":"d365uLa7l8qN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Fit the Multiple Linear Regression Model"],"metadata":{"id":"g0vIzqSZmFgS"}},{"cell_type":"code","source":["# Fit model with 3 predictors\n","model <- lm(score ~ study_hours + attendance + sleep_hours, data = df)\n","\n","# View summary\n","summary(model)"],"metadata":{"id":"uHd-TCmemGZ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample Output (abridged)\n","\n","```\n","Coefficients:\n","                Estimate Std. Error t value Pr(>|t|)    \n","(Intercept)      48.5612     4.1374  11.74  < 2e-16 ***\n","study_hours       1.9643     0.2017   9.74  < 2e-16 ***\n","attendance        0.5041     0.0471  10.70  < 2e-16 ***\n","sleep_hours       1.4936     0.4213   3.54   0.0007 ***\n","\n","Multiple R-squared:  0.935\n","Adjusted R-squared:  0.933\n","F-statistic: 459.2 on 3 and 96 DF,  p-value: < 2.2e-16\n","```\n","\n","---\n","\n","### Interpretation of Coefficients\n","\n","| Variable     | Estimate | Meaning                                                                                        |\n","| ------------ | -------- | ---------------------------------------------------------------------------------------------- |\n","| Intercept    | 48.56    | Expected score when all predictors = 0 (baseline)                                              |\n","| study\\_hours | 1.96     | For each additional hour studied, score increases by \\~1.96 (holding other variables constant) |\n","| attendance   | 0.50     | For each 1% increase in attendance, score increases by \\~0.50                                  |\n","| sleep\\_hours | 1.49     | For each additional hour of sleep, score increases by \\~1.49                                   |\n","\n","---\n","\n","### Model Quality Metrics\n","\n","* **R-squared = 0.935**\n","  → Model explains 93.5% of the variance in test scores.\n","\n","* **Adjusted R-squared = 0.933**\n","  → Adjusts R² for number of predictors; penalizes overfitting.\n","\n","* **F-statistic = 459.2, p < 0.001**\n","  → The model as a whole is statistically significant.\n","\n","---\n","\n","## Checking for Multicollinearity\n","\n","Highly correlated predictors distort estimates. Use **Variance Inflation Factor (VIF)**:"],"metadata":{"id":"3iD50XmTmRil"}},{"cell_type":"code","source":["install.packages(\"car\")  # Run once if not installed\n","library(car)\n","vif(model)"],"metadata":{"id":"b46rjHNVmbGL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["| VIF Value | Interpretation                  |\n","| --------- | ------------------------------- |\n","| 1-5       | Acceptable                      |\n","| > 5       | Potential multicollinearity     |\n","| > 10      | Serious multicollinearity issue |\n","\n","---\n","\n","## Model Diagnostics\n","\n","### Check regression assumptions using diagnostic plots:"],"metadata":{"id":"vbXXvAVYn7y6"}},{"cell_type":"code","source":["par(mfrow = c(2, 2))\n","plot(model)"],"metadata":{"id":"s5y62NFFn80-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You'll see:\n","\n","1. **Residuals vs Fitted** → Linearity & homoscedasticity\n","2. **Normal Q-Q** → Normality of residuals\n","3. **Scale-Location** → Variance consistency\n","4. **Residuals vs Leverage** → Influential observations\n","\n","---\n","\n","## Predicting New Data"],"metadata":{"id":"n8Yiw8qZoP30"}},{"cell_type":"code","source":["# New student data\n","new_student <- data.frame(\n","  study_hours = 12,\n","  attendance = 95,\n","  sleep_hours = 8\n",")\n","\n","# Predict score with confidence and prediction intervals\n","predict(model, newdata = new_student, interval = \"confidence\")\n","predict(model, newdata = new_student, interval = \"prediction\")"],"metadata":{"id":"vp0_B75KoQzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary of Concepts\n","\n","| Term                | Description                                                        |\n","| ------------------- | ------------------------------------------------------------------ |\n","| Coefficient         | Effect of a 1-unit change in a predictor (holding others constant) |\n","| R-squared           | % of outcome explained by model                                    |\n","| Adjusted R-squared  | R² corrected for number of predictors                              |\n","| VIF                 | Multicollinearity check                                            |\n","| Confidence Interval | Predicts average outcome                                           |\n","| Prediction Interval | Predicts new individual outcome                                    |\n","\n","---\n","\n","## Practice Tip\n","\n","Use **`step()`** for model selection:"],"metadata":{"id":"OqfP6zyCofrY"}},{"cell_type":"code","source":["step(model, direction = \"both\")"],"metadata":{"id":"1OIs9yVLoghj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It selects the best subset of predictors based on **AIC**, helping prevent overfitting."],"metadata":{"id":"ds15MCvdlGMo"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"MQNnRLZEoxKK"}},{"cell_type":"markdown","source":["# **6.8 Nonlinear Regression**\n","\n","## Objective\n","\n","Unlike linear regression which models a straight-line relationship, **nonlinear regression** is used when the relationship between the dependent and independent variable is **curved, exponential, or otherwise non-linear**.\n","\n","This technique is essential when:\n","\n","* Patterns in residuals suggest a curved relationship\n","* Theoretical models suggest exponential, logarithmic, or polynomial behavior\n","\n","---\n","\n","## When to Use Nonlinear Regression\n","\n","Use nonlinear regression if:\n","\n","* The relationship between predictors and outcome is **non-additive** or **non-constant**\n","* Residual plots from linear regression show **systematic patterns** (i.e., not random)\n","\n","---\n","\n","## Common Types of Nonlinear Models\n","\n","| Type            | Model Form                                                                |\n","| --------------- | ------------------------------------------------------------------------- |\n","| **Exponential** | $Y = \\beta_0 \\cdot e^{\\beta_1 X} + \\epsilon$                              |\n","| **Logarithmic** | $Y = \\beta_0 + \\beta_1 \\cdot \\log(X) + \\epsilon$                          |\n","| **Power**       | $Y = \\beta_0 X^{\\beta_1} + \\epsilon$                                      |\n","| **Polynomial**  | $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\cdots + \\beta_k X^k + \\epsilon$ |\n","\n","---\n","\n","## Fitting a Nonlinear Model in R\n","\n","Let's begin with a **polynomial regression** example, which is technically still linear in parameters, but captures non-linear relationships.\n","\n","---\n","\n","### **A. Polynomial Regression Example**"],"metadata":{"id":"3JaZF3B9p5f0"}},{"cell_type":"code","source":["# Simulate a nonlinear relationship\n","set.seed(123)\n","x <- seq(0, 10, length.out = 100)\n","y <- 2 + 0.5 * x - 0.1 * x^2 + rnorm(100, sd = 1.5)\n","df <- data.frame(x, y)\n","\n","# Fit polynomial regression (degree 2)\n","model_poly <- lm(y ~ x + I(x^2), data = df)\n","\n","# Summary\n","summary(model_poly)"],"metadata":{"id":"dibvMPY6p7HL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Explanation\n","\n","* `I(x^2)` tells R to include `x^2` as a separate predictor.\n","* This captures curvature in the data (i.e., allows a U-shape or inverted-U).\n","\n","---\n","\n","### **Plot the Polynomial Fit**"],"metadata":{"id":"pjxmNorBqK-K"}},{"cell_type":"code","source":["plot(x, y, main = \"Polynomial Regression Fit\", pch = 19)\n","curve(predict(model_poly, newdata = data.frame(x = x)), add = TRUE, col = \"blue\", lwd = 2)"],"metadata":{"id":"PIOWAhpDqL5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample Output\n","\n","```\n","Coefficients:\n","(Intercept)     2.07  \n","x              0.56  \n","I(x^2)        -0.09  \n","R-squared      0.85\n","```\n","\n","* Interpretation:\n","  The model captures curvature; the negative coefficient of `x^2` indicates a **downward-opening parabola**.\n","\n","---\n","\n","### **B. Truly Nonlinear Regression Using `nls()`**\n","\n","The `nls()` function fits models where the relationship is **nonlinear in the parameters**.\n","\n","#### Example: Exponential Model\n","\n","$$\n","Y = a \\cdot e^{bX}\n","$$\n"],"metadata":{"id":"6Vd-LVeqqaQk"}},{"cell_type":"code","source":["# Simulate exponential data\n","set.seed(123)\n","x <- 1:100\n","y <- 5 * exp(0.05 * x) + rnorm(100, sd = 20)\n","df2 <- data.frame(x, y)\n","\n","# Fit nonlinear model using nls\n","model_nls <- nls(y ~ a * exp(b * x), data = df2, start = list(a = 1, b = 0.01))\n","\n","# Summary\n","summary(model_nls)"],"metadata":{"id":"9HfCXJZyqbE_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Explanation\n","\n","* `nls()` stands for **Nonlinear Least Squares**\n","* `start` specifies initial guesses for the parameters\n","* `a` and `b` are fitted using iterative optimization\n","\n","---\n","\n","### Plotting the Exponential Fit"],"metadata":{"id":"72nn8azHrEag"}},{"cell_type":"code","source":["plot(df2$x, df2$y, main = \"Exponential Fit\", pch = 16)\n","lines(df2$x, predict(model_nls), col = \"blue\", lwd = 2)"],"metadata":{"id":"WIFzr5t9rFlp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## When Using `nls()`, Keep in Mind:\n","\n","* Requires good starting values\n","* Fails to converge if the model is poorly specified\n","* Works best with **known functional forms**\n","\n","---\n","\n","## Model Comparison\n","\n","You can still compare models using:\n","\n","* **Residual Sum of Squares (RSS)**\n","* **AIC** (via `AIC()` function)\n","* **Visual fit and residual patterns**\n","\n","---\n","\n","## Summary Table\n","\n","| Model       | Use Case                            | Fit Function              |\n","| ----------- | ----------------------------------- | ------------------------- |\n","| Polynomial  | Curved linear fits (e.g., parabola) | `lm(y ~ x + I(x^2))`      |\n","| Exponential | Growth or decay                     | `nls(y ~ a * exp(b * x))` |\n","| Logarithmic | Rapid rise that levels off          | `lm(y ~ log(x))`          |\n","| Power       | Scale-invariant patterns            | `nls(y ~ a * x^b)`        |\n","\n","---\n","\n","## Tips for Practice\n","\n","1. **Start with linear** — check residuals.\n","2. **Visualize** your data first — does it look curved, exponential, etc.?\n","3. **Choose the right model form** based on theory or residuals.\n","4. **Always inspect plots** after fitting nonlinear models.\n"],"metadata":{"id":"5DsMja5xpllK"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"EXLAwgdFrXaQ"}},{"cell_type":"markdown","source":["# **6.9 Logistic Regression**\n","\n","## Objective\n","\n","**Logistic Regression** is used when the **dependent variable is binary or categorical**. It estimates the probability that an observation belongs to a particular category (e.g., Yes/No, Success/Failure, 0/1).\n","\n","It is one of the most powerful tools for **classification problems**.\n","\n","---\n","\n","## When to Use Logistic Regression\n","\n","* When the outcome variable is **binary** (e.g., disease vs. no disease)\n","* When you're interested in **modeling the probability** of success\n","* When you want to predict **membership in a group**\n","\n","---\n","\n","## Key Differences from Linear Regression\n","\n","| Feature          | Linear Regression  | Logistic Regression            |\n","| ---------------- | ------------------ | ------------------------------ |\n","| Outcome variable | Continuous         | Binary (0 or 1)                |\n","| Output           | Numeric prediction | Probability (0-1)              |\n","| Function used    | Linear             | Sigmoid (logistic)             |\n","| Interpretation   | Y increases by...  | Odds of outcome increase by... |\n","\n","---\n","\n","## The Logistic Model\n","\n","$$\n","\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k\n","$$\n","\n","Where:\n","\n","* $p$: probability of the outcome being 1 (success)\n","* $\\text{logit}(p)$: natural log of odds\n","* $X_1, X_2, \\ldots$: predictors\n","* $\\beta_0, \\beta_1, \\ldots$: model coefficients\n","\n","The inverse of the logit gives the **predicted probability**:\n","\n","$$\n","p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n","$$\n","\n","---\n","\n","## Example Use Case\n","\n","You want to **predict whether a customer will buy** (1) or not buy (0) a product based on:\n","\n","* Age\n","* Income\n","* Website clicks\n","\n","---\n","\n","## Step-by-Step in R\n","\n","### Step 1: Simulate Binary Data"],"metadata":{"id":"u-0UxOFZr-kL"}},{"cell_type":"code","source":["set.seed(2025)\n","\n","# Simulate predictor variables\n","age <- rnorm(200, mean = 40, sd = 10)\n","income <- rnorm(200, mean = 50000, sd = 12000)\n","clicks <- rpois(200, lambda = 4)\n","\n","# Create probability using logistic model\n","logit <- -8 + 0.1 * age + 0.0001 * income + 0.5 * clicks\n","p <- 1 / (1 + exp(-logit))   # Logistic function\n","purchase <- rbinom(200, 1, prob = p)  # Outcome: 0 or 1\n","\n","# Create data frame\n","df <- data.frame(purchase, age, income, clicks)"],"metadata":{"id":"6_EtdwsesGYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Fit Logistic Regression"],"metadata":{"id":"3eBrptY9sQEp"}},{"cell_type":"code","source":["# Fit logistic regression model\n","model <- glm(purchase ~ age + income + clicks, data = df, family = binomial)\n","\n","# Summary of the model\n","summary(model)"],"metadata":{"id":"b439K8uxsQ2D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample Output (abridged)\n","\n","```\n","Coefficients:\n","            Estimate Std. Error z value Pr(>|z|)    \n","(Intercept) -8.12312   1.45612  -5.58   <2e-07 ***\n","age          0.10245   0.03115   3.29   0.001 ***\n","income       0.00011   0.00004   2.75   0.006 **\n","clicks       0.50215   0.11320   4.43   0.00001 ***\n","```\n","\n","---\n","\n","### Interpretation of Coefficients\n","\n","* All variables are **statistically significant** (p < 0.05)\n","* **age**: each additional year increases the log-odds of purchasing\n","* **income**: higher income slightly increases purchase probability\n","* **clicks**: more clicks significantly increase purchase likelihood\n","\n","---\n","\n","## Predicting Probabilities"],"metadata":{"id":"84kXnIwosqYt"}},{"cell_type":"code","source":["# Predict probabilities for original dataset\n","predicted_probs <- predict(model, type = \"response\")\n","\n","# Show first 5\n","head(predicted_probs)"],"metadata":{"id":"kJnRYKfXsrJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Returns predicted probability of `purchase = 1`\n","\n","---\n","\n","## Classifying Outcomes\n","\n","Use a threshold (e.g., 0.5) to make binary predictions:\n"],"metadata":{"id":"JzB3ErMws3lJ"}},{"cell_type":"code","source":["predicted_class <- ifelse(predicted_probs > 0.5, 1, 0)\n","table(Predicted = predicted_class, Actual = df$purchase)"],"metadata":{"id":"cYJ-Nzy0s4sV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## Model Evaluation Metrics\n","\n","Use the **confusion matrix** to evaluate classification:\n","\n","* **True Positive (TP)**: Predicted 1 and actually 1\n","* **True Negative (TN)**: Predicted 0 and actually 0\n","* **False Positive (FP)**: Predicted 1 but actually 0\n","* **False Negative (FN)**: Predicted 0 but actually 1\n","\n","From the confusion matrix, compute:\n","\n","* **Accuracy**: $\\frac{TP + TN}{Total}$\n","* **Sensitivity (Recall)**: $\\frac{TP}{TP + FN}$\n","* **Specificity**: $\\frac{TN}{TN + FP}$\n","\n","---\n","\n","## Visualizing Predicted Probabilities"],"metadata":{"id":"u95YimFftHd5"}},{"cell_type":"code","source":["library(ggplot2)\n","\n","ggplot(df, aes(x = age, y = predicted_probs)) +\n","  geom_point() +\n","  geom_smooth(method = \"loess\") +\n","  labs(title = \"Predicted Probability of Purchase by Age\",\n","       y = \"Predicted Probability\")"],"metadata":{"id":"c1ngq829tIeZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary of Concepts\n","\n","| Term                              | Meaning                              |\n","| --------------------------------- | ------------------------------------ |\n","| Logit                             | Log odds of success                  |\n","| Logistic function                 | Converts logit to probability        |\n","| `glm(..., family = binomial)`     | Fits logistic regression             |\n","| `predict(..., type = \"response\")` | Gets predicted probabilities         |\n","| Confusion Matrix                  | Compares predicted vs actual classes |\n","\n","---\n","\n","## Important Notes\n","\n","* Logistic regression **does not assume normality** of predictors or residuals\n","* Multicollinearity is still a concern (check with `vif()`)\n","* You can use **stepwise regression** with `step()` to select best predictors\n","* For more than two outcomes, use **multinomial or ordinal logistic regression**\n"],"metadata":{"id":"hPYXs0BarZ5B"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"do6-dCO3tix_"}},{"cell_type":"markdown","source":["# **7.0 One-Way Analysis of Variance (ANOVA)**\n","\n","## Objective\n","\n","**One-Way ANOVA** is used to compare the **means of three or more groups** to determine whether at least one group is significantly different from the others.\n","\n","It answers the question:\n","\n","> \"Is there a statistically significant difference between group means?\"\n","\n","---\n","\n","## Real-World Examples\n","\n","* Comparing **test scores** across three different teaching methods\n","* Evaluating **sales** across four store regions\n","* Testing if **blood pressure** varies between different treatment groups\n","\n","---\n","\n","## Key Terminology\n","\n","| Term                       | Description                                        |\n","| -------------------------- | -------------------------------------------------- |\n","| **Factor**                 | A categorical independent variable (e.g., “group”) |\n","| **Levels**                 | The categories within a factor (e.g., A, B, C)     |\n","| **Response variable**      | Numeric outcome being measured                     |\n","| **Between-group variance** | Variance due to differences between group means    |\n","| **Within-group variance**  | Variance within each group (random noise)          |\n","\n","---\n","\n","## Hypotheses in One-Way ANOVA\n","\n","* **Null Hypothesis $H_0$**: All group means are equal\n","  $\\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k$\n","\n","* **Alternative Hypothesis $H_A$**: At least one group mean is different\n","\n","---\n","\n","## Assumptions of ANOVA\n","\n","1. **Independence** of observations\n","2. **Normality** of the response variable within each group\n","3. **Homogeneity of variance** (equal variances across groups)\n","\n","---\n","\n","## Step-by-Step in R\n","\n","Let's say we want to test whether **exam scores** differ between **3 different teaching methods**.\n","\n","---\n","\n","### Step 1: Simulate Data"],"metadata":{"id":"CQk4yS1W1cbu"}},{"cell_type":"code","source":["set.seed(707)\n","\n","# Simulate exam scores for 3 teaching methods\n","method <- factor(rep(c(\"Traditional\", \"Online\", \"Hybrid\"), each = 30))\n","\n","score <- c(\n","  rnorm(30, mean = 75, sd = 5),     # Traditional\n","  rnorm(30, mean = 80, sd = 5),     # Online\n","  rnorm(30, mean = 78, sd = 5)      # Hybrid\n",")\n","\n","df <- data.frame(method, score)"],"metadata":{"id":"7j-Y7oh81dYI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Visualize the Group Means"],"metadata":{"id":"lW9VIk4A1tve"}},{"cell_type":"code","source":["boxplot(score ~ method, data = df,\n","        main = \"Exam Scores by Teaching Method\",\n","        ylab = \"Score\", xlab = \"Method\", col = \"skyblue\")"],"metadata":{"id":"TIFBOjzU1up-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Run One-Way ANOVA"],"metadata":{"id":"d3bDN7J72JYn"}},{"cell_type":"code","source":["anova_model <- aov(score ~ method, data = df)\n","summary(anova_model)"],"metadata":{"id":"S1CjB0mS2Kad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample Output\n","\n","```\n","            Df Sum Sq Mean Sq F value Pr(>F)    \n","method       2  586.4   293.2   12.12  1.1e-05 ***\n","Residuals   87 2104.1    24.2                   \n","```\n","\n","---\n","\n","## Interpreting the ANOVA Table\n","\n","| Column      | Meaning                                                       |\n","| ----------- | ------------------------------------------------------------- |\n","| **Df**      | Degrees of freedom (2 for 3 groups, n-1 for residuals)        |\n","| **Sum Sq**  | Total variability (between & within groups)                   |\n","| **Mean Sq** | Average variability (Sum Sq ÷ Df)                             |\n","| **F value** | Test statistic — large F means more difference between groups |\n","| **Pr(>F)**  | p-value — small p indicates significant difference            |\n","\n","**Conclusion**:\n","\n","* p-value < 0.05 → Reject null hypothesis → At least one group mean is different\n","\n","---\n","\n","## Step 4: Post-Hoc Test (Tukey's HSD)\n","\n","If ANOVA is significant, you use **post-hoc tests** to determine **which groups differ**."],"metadata":{"id":"xUSC6DFN2anu"}},{"cell_type":"code","source":["TukeyHSD(anova_model)"],"metadata":{"id":"bG4N5Wk-2boL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Example output:\n","\n","```\n","              diff      lwr      upr     p adj\n","Online-Traditional  5.2     2.4      8.0     0.001\n","Hybrid-Traditional  3.4     0.6      6.2     0.015\n","Hybrid-Online      -1.8    -4.6      1.0     0.290\n","```\n","\n","Interpretation:\n","\n","* \"Online vs Traditional\": significant difference (p = 0.001)\n","* \"Hybrid vs Online\": no significant difference (p = 0.29)\n","\n","---\n","\n","## Step 5: Check ANOVA Assumptions\n","\n","### A. Residual Normality"],"metadata":{"id":"mSY8p3TA4rG7"}},{"cell_type":"code","source":["qqnorm(residuals(anova_model))\n","qqline(residuals(anova_model))"],"metadata":{"id":"bLhwL8jz4wlv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### B. Homogeneity of Variance"],"metadata":{"id":"ym1kWfY342e_"}},{"cell_type":"code","source":["plot(anova_model, which = 1)  # Residuals vs Fitted"],"metadata":{"id":"DNPx_PEO43bZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OR use **Levene's Test** from `car` package:"],"metadata":{"id":"eR7DiGHt5Cz-"}},{"cell_type":"code","source":["# install.packages(\"car\")\n","library(car)\n","leveneTest(score ~ method, data = df)"],"metadata":{"id":"2s3liGqH5DkE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## Summary of Concepts\n","\n","| Term          | Meaning                                               |\n","| ------------- | ----------------------------------------------------- |\n","| One-Way ANOVA | Compares means across 3+ groups                       |\n","| `aov()`       | R function to fit ANOVA model                         |\n","| `TukeyHSD()`  | Post-hoc test to find pairwise group differences      |\n","| F-value       | Signal-to-noise ratio                                 |\n","| p-value       | Probability of observing result under null hypothesis |\n","\n","---\n","\n","## Conclusion\n","\n","**One-Way ANOVA** helps answer:\n","\n","> “Are there any significant differences between group means?”\n","\n","If significant, follow up with **post-hoc analysis** to find **which** groups differ.\n"],"metadata":{"id":"XBZtLSjQtlbO"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"3hrmIkTx5VXG"}},{"cell_type":"markdown","source":["# **7.1 One-Way ANOVA (Expanded)**\n","\n","## Objective\n","\n","To understand:\n","\n","* The **mathematical structure** of one-way ANOVA\n","* How to **test and validate its assumptions**\n","* How to **apply ANOVA to real-world data**\n","* How to **report results in a statistically sound way**\n","\n","---\n","\n","## 1. The Mathematical Logic Behind One-Way ANOVA\n","\n","Let's break down how one-way ANOVA works:\n","\n","You divide the **total variation** in the outcome into:\n","\n","* **Between-group variation** (explained by group means)\n","* **Within-group variation** (random error)\n","\n","This is formalized as:\n","\n","$$\n","\\text{Total SS} = \\text{Between-group SS} + \\text{Within-group SS}\n","$$\n","\n","The F-statistic is then:\n","\n","$$\n","F = \\frac{\\text{MS}_{\\text{between}}}{\\text{MS}_{\\text{within}}}\n","= \\frac{\\text{SS}_{\\text{between}} / df_{\\text{between}}}{\\text{SS}_{\\text{within}} / df_{\\text{within}}}\n","$$\n","\n","If the between-group variance is significantly larger than within-group variance, **the group means are likely different**.\n","\n","---\n","\n","## 2. Use Case: Iris Dataset\n","\n","Let's use the built-in `iris` dataset to compare **petal length** across 3 species:"],"metadata":{"id":"Dlq5EqtM5YtA"}},{"cell_type":"code","source":["# Load iris dataset\n","data(iris)\n","\n","# View the first few rows\n","head(iris)"],"metadata":{"id":"bcjH9NOT5Xcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 1: Boxplot Visualization\n"],"metadata":{"id":"78mH_ghP6bec"}},{"cell_type":"code","source":["boxplot(Petal.Length ~ Species, data = iris,\n","        col = c(\"lightblue\", \"lightgreen\", \"lightpink\"),\n","        main = \"Petal Length by Species\",\n","        ylab = \"Petal Length\")"],"metadata":{"id":"uyt0tGyB6ceH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This gives a visual hint: Species may differ in petal length.\n","\n","---\n","\n","### Step 2: Run One-Way ANOVA"],"metadata":{"id":"Pi5_l1T76n1D"}},{"cell_type":"code","source":["model <- aov(Petal.Length ~ Species, data = iris)\n","summary(model)"],"metadata":{"id":"fhPZ1qRg6osz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Sample Output\n","\n","```\n","            Df Sum Sq Mean Sq F value Pr(>F)    \n","Species      2  437.1  218.55   1180.2 <2e-16 ***\n","Residuals  147   27.2    0.18                   \n","```\n","\n","**Interpretation**:\n","\n","* Extremely high F-value and p < 0.001\n","* Strong evidence that **at least one species** has a different mean petal length\n","\n","---\n","\n","### Step 3: Tukey HSD Post-Hoc Test"],"metadata":{"id":"z0nTashV605c"}},{"cell_type":"code","source":["TukeyHSD(model)"],"metadata":{"id":"0JW0XKj361yR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Sample Output\n","\n","```\n","                diff       lwr       upr     p adj\n","versicolor-setosa   2.798      2.60      2.99   <0.001\n","virginica-setosa    4.090      3.89      4.29   <0.001\n","virginica-versicolor 1.292     1.09      1.49   <0.001\n","```\n","\n","All species have significantly different petal lengths.\n","\n","---\n","\n","### Step 4: Plot Tukey HSD Results"],"metadata":{"id":"m1shLmLD7CkR"}},{"cell_type":"code","source":["plot(TukeyHSD(model), las = 1)"],"metadata":{"id":"7lZ689GD7DV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This shows the confidence intervals for differences between means.\n","\n","---\n","\n","## 3. Validating ANOVA Assumptions\n","\n","### A. Check for Normality of Residuals"],"metadata":{"id":"WD2ltp677UuQ"}},{"cell_type":"code","source":["qqnorm(residuals(model))\n","qqline(residuals(model))"],"metadata":{"id":"KLdVL4f57VZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Points should fall along the line\n","* If major departures exist, normality may be violated\n","\n","### B. Check for Equal Variance"],"metadata":{"id":"3pHKbl4N7pmj"}},{"cell_type":"code","source":["plot(model, which = 1)  # Residuals vs Fitted"],"metadata":{"id":"GLJhTWtY7qlr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also, use **Levene's Test**:"],"metadata":{"id":"5gDCRMQS72A3"}},{"cell_type":"code","source":["library(car)\n","leveneTest(Petal.Length ~ Species, data = iris)"],"metadata":{"id":"GNMUN8iB726-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Reporting Results (Academic Style)\n","\n","> A one-way ANOVA revealed that there were significant differences in petal length between species:\n","> F(2, 147) = 1180.2, p < .001.\n","> Post hoc analysis using Tukey’s HSD indicated that all pairwise differences were statistically significant.\n","\n","---\n","\n","## Summary Table\n","\n","| Step        | Description                                           |\n","| ----------- | ----------------------------------------------------- |\n","| Visualize   | Use boxplots to inspect group differences             |\n","| Fit ANOVA   | `aov(response ~ group, data)`                         |\n","| Post-hoc    | `TukeyHSD()` identifies specific pairwise differences |\n","| Assumptions | Check residual normality and equal variance           |\n","| Report      | Include F-statistic, df, and p-value                  |\n","\n","---\n","\n","## Best Practices\n","\n","* Always **visualize** your data first\n","* If assumptions are violated:\n","\n","  * Use **Kruskal-Wallis Test** (non-parametric)\n","* For **more than one factor**, use **two-way ANOVA** (coming next)\n","* Report **effect sizes** if needed (e.g., η²)\n","\n","---"],"metadata":{"id":"qVm1JMV-5Z8t"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"nRyZItYP8H-Q"}},{"cell_type":"markdown","source":["# **7.2 Two-Way ANOVA**\n","\n","## Objective\n","\n","**Two-Way ANOVA** allows you to examine how **two categorical independent variables (factors)** affect a **single continuous outcome**, and whether there's an **interaction** between them.\n","\n","This is especially powerful when studying:\n","\n","* The **individual effects** of each factor\n","* The **combined effect** when the two factors interact\n","\n","---\n","\n","## Real-World Example\n","\n","You want to analyze:\n","\n","> The effect of **teaching method** (*Traditional*, *Online*) and **gender** (*Male*, *Female*) on **student performance**.\n","\n","You're testing:\n","\n","1. **Main effect** of teaching method\n","2. **Main effect** of gender\n","3. **Interaction effect**: Does the effectiveness of a method depend on gender?\n","\n","---\n","\n","## Model Structure\n","\n","The Two-Way ANOVA model:\n","\n","$$\n","Y = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\n","$$\n","\n","Where:\n","\n","* $Y$: response variable\n","* $\\alpha_i$: effect of factor A (e.g., method)\n","* $\\beta_j$: effect of factor B (e.g., gender)\n","* $(\\alpha\\beta)_{ij}$: interaction effect\n","* $\\varepsilon$: random error\n","\n","---\n","\n","## Step-by-Step in R\n","\n","We'll simulate a dataset with two factors:\n","\n","* `method`: Traditional or Online\n","* `gender`: Male or Female\n","\n","---\n","\n","### Step 1: Simulate the Data"],"metadata":{"id":"yYZjU7Fr8KoA"}},{"cell_type":"code","source":["set.seed(2025)\n","\n","# Define factors\n","method <- rep(c(\"Traditional\", \"Online\"), each = 40)\n","gender <- rep(c(\"Male\", \"Female\"), times = 40)\n","\n","# Create outcome with interaction\n","score <- c(\n","  rnorm(20, mean = 70, sd = 5),   # Traditional, Male\n","  rnorm(20, mean = 75, sd = 5),   # Traditional, Female\n","  rnorm(20, mean = 76, sd = 5),   # Online, Male\n","  rnorm(20, mean = 80, sd = 5)    # Online, Female\n",")\n","\n","df <- data.frame(method = factor(method), gender = factor(gender), score)"],"metadata":{"id":"4AMNmche8LVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Visualize with Interaction Plot"],"metadata":{"id":"e3N55gZcvTMf"}},{"cell_type":"code","source":["interaction.plot(df$method, df$gender, df$score,\n","                 col = c(\"blue\", \"darkgreen\"), lwd = 2,\n","                 main = \"Interaction: Method vs Gender\",\n","                 ylab = \"Average Score\")"],"metadata":{"id":"NF6H1ZtJvUFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **Parallel lines** → no interaction\n","* **Non-parallel lines** → interaction likely present\n","\n","---\n","\n","### Step 3: Run Two-Way ANOVA"],"metadata":{"id":"MIyilGQwvgKF"}},{"cell_type":"code","source":["model <- aov(score ~ method * gender, data = df)\n","summary(model)"],"metadata":{"id":"QbPE_C8Jvg-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Output Interpretation:\n","\n","```\n","                 Df Sum Sq Mean Sq F value  Pr(>F)    \n","method            1  130.5  130.50   5.89   0.017*  \n","gender            1  186.3  186.25   8.40   0.005**\n","method:gender     1   25.8   25.78   1.16   0.285    \n","Residuals        76 1683.6   22.15                   \n","```\n","\n","---\n","\n","### How to Interpret Each Line:\n","\n","| Term            | Meaning                                                   |\n","| --------------- | --------------------------------------------------------- |\n","| `method`        | Main effect of teaching method (significant if p < 0.05)  |\n","| `gender`        | Main effect of gender                                     |\n","| `method:gender` | Interaction effect — does method effect depend on gender? |\n","| `Residuals`     | Unexplained variance                                      |\n","\n","---\n","\n","## Post-Hoc Tests\n","\n","For pairwise comparisons (if factors have >2 levels):"],"metadata":{"id":"RQltAzxHvxDU"}},{"cell_type":"code","source":["TukeyHSD(model)"],"metadata":{"id":"xP1wo1S-vx_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If only 2 levels per factor (as in our case), direct interpretation from summary is sufficient.\n","\n","---\n","\n","## Model Without Interaction (Optional)\n","\n","If interaction is not significant, you may refit the model **without interaction**:"],"metadata":{"id":"99UvoTnawE04"}},{"cell_type":"code","source":["model_main <- aov(score ~ method + gender, data = df)\n","summary(model_main)"],"metadata":{"id":"mS-NaswUwGJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare both models using **AIC** or **Adjusted R²** if needed.\n","\n","---\n","\n","## Check Assumptions\n","\n","### 1. Residual Normality"],"metadata":{"id":"yKcZTmk_wRvk"}},{"cell_type":"code","source":["qqnorm(residuals(model))\n","qqline(residuals(model))"],"metadata":{"id":"h7Z7ByhHwS51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Homogeneity of Variance"],"metadata":{"id":"xd3yGmpJwgr-"}},{"cell_type":"code","source":["plot(model, which = 1)  # Residuals vs Fitted"],"metadata":{"id":"pX1I-JrhwhfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Or use:"],"metadata":{"id":"KDdtseLFwxmK"}},{"cell_type":"code","source":["install.packages('car')\n","library(car)\n","leveneTest(score ~ interaction(method, gender), data = df)"],"metadata":{"id":"DIXfECd5w3Ug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary of Key Concepts\n","\n","| Concept              | Description                                      |\n","| -------------------- | ------------------------------------------------ |\n","| Two-Way ANOVA        | Tests two factors and their interaction          |\n","| Main Effect          | Influence of one factor regardless of the other  |\n","| Interaction          | Whether one factor’s effect depends on the other |\n","| `aov(y ~ A * B)`     | Includes both main effects and interaction       |\n","| `interaction.plot()` | Visualize possible interaction                   |\n","\n","---\n","\n","## Conclusion\n","\n","Two-way ANOVA helps uncover not just **whether** factors matter, but **how** they combine. Interaction terms are crucial in real-world modeling, especially in fields like:\n","\n","* Marketing (region \\* campaign)\n","* Medicine (treatment \\* gender)\n","* Education (method \\* prior knowledge)\n"],"metadata":{"id":"7J4wtfbd8Mj9"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"MxzaVb92xZ-T"}},{"cell_type":"markdown","source":["# **7.3: Analysis with Categorical and Quantitative Variables - FEV Dataset**\n","\n","---\n","\n","## Objective\n","\n","To demonstrate how to analyze **combined effects** of **categorical** and **continuous (quantitative)** variables using linear modeling techniques (like ANOVA and regression), with a real dataset.\n","\n","We'll use the **FEV dataset**, which contains lung function measurements for children and adolescents.\n","\n","---\n","\n","## About the FEV Dataset\n","\n","The dataset comes from medical studies measuring:\n","\n","| Variable | Description                                         |\n","| -------- | --------------------------------------------------- |\n","| `FEV`    | Forced Expiratory Volume (lung capacity, in liters) |\n","| `age`    | Age (years)                                         |\n","| `height` | Height (inches)                                     |\n","| `gender` | Male or Female                                      |\n","| `smoke`  | Smoking status: 1 (Smoker), 0 (Non-smoker)          |\n","\n","---\n","\n","## Goal\n","\n","Analyze the effect of:\n","\n","* **Smoking status** (categorical)\n","* **Age and height** (quantitative)\n","* On **FEV** (lung function)\n","\n","Specifically:\n","\n","* Does smoking reduce FEV?\n","* Do age and height explain lung capacity?\n","* Is there an interaction between smoking and age?\n","\n","---\n","\n","## Step-by-Step in R\n","\n","### Step 1: Load the Data"],"metadata":{"id":"wKCaEgMDxcEf"}},{"cell_type":"code","source":["url <- \"https://raw.githubusercontent.com/GTPB/PSLS20/master/data/fev.txt\"\n","fev <- read.table(url, header = TRUE)\n","head(fev)"],"metadata":{"id":"X3lsEL0BbTAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Basic Data Exploration"],"metadata":{"id":"ZnQvd38pykjv"}},{"cell_type":"code","source":["str(fev)\n","summary(fev)\n","table(fev$smoking)"],"metadata":{"id":"wDBy2D8wypVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check for:\n","\n","* Balance between smokers and non-smokers\n","* Age and height distribution\n","\n","---\n","\n","### Step 3: Visualize FEV by Smoking Status"],"metadata":{"id":"q_J9IjVVy7J-"}},{"cell_type":"code","source":["boxplot(fev ~ smoking, data = fev,\n","        main = \"FEV by Smoking Status\",\n","        names = c(\"Non-Smoker\", \"Smoker\"),\n","        col = c(\"lightgreen\", \"tomato\"),\n","        ylab = \"FEV (liters)\")"],"metadata":{"id":"Vna1mQUey8gF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This shows differences in lung capacity between smokers and non-smokers.\n","\n","---\n","\n","### Step 4: Fit Linear Model with Both Types of Predictors\n"],"metadata":{"id":"T31fHSDQzKSx"}},{"cell_type":"code","source":["model <- lm(fev ~ smoking + age + height, data = fev)\n","summary(model)"],"metadata":{"id":"5-YRr4VXzL-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample Output\n","\n","```\n","Coefficients:\n","              Estimate Std. Error t value Pr(>|t|)    \n","(Intercept)    -4.1892     0.7575  -5.53  2.6e-07 ***\n","smoke           -0.2342     0.1021  -2.29   0.023 *  \n","age              0.1747     0.0195   8.96  < 2e-16 ***\n","height           0.0901     0.0127   7.09  3.2e-12 ***\n","```\n","\n","---\n","\n","### Interpretation\n","\n","| Term      | Meaning                                                             |\n","| --------- | ------------------------------------------------------------------- |\n","| Intercept | Expected FEV when all predictors are 0 (not directly interpretable) |\n","| `smoke`   | Smokers have **0.23 liters lower FEV**, on average (p = 0.023)      |\n","| `age`     | Each additional year of age increases FEV by \\~0.17 liters          |\n","| `height`  | Each additional inch increases FEV by \\~0.09 liters                 |\n","\n","All predictors are **statistically significant**\n","\n","---\n","\n","### Step 5: Interaction Effect: Smoke x Age\n","\n","Let's check if **smoking impacts FEV differently by age**:"],"metadata":{"id":"bvXHOGekzXIe"}},{"cell_type":"code","source":["model2 <- lm(fev ~ smoking * age + height, data = fev)\n","summary(model2)"],"metadata":{"id":"rOsKylVKzYK_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Output Interpretation (example)\n","\n","```\n","smoke:age -0.0164, p = 0.08\n","```\n","\n","If the interaction term is significant:\n","\n","* It means smoking has a **greater negative effect at older ages** (or varies by age)\n","\n","If **not significant**, you may choose to drop the interaction.\n","\n","---\n","\n","### Step 6: Plot the Interaction"],"metadata":{"id":"TXgQAUNdzrhY"}},{"cell_type":"code","source":["library(ggplot2)\n","\n","ggplot(fev, aes(x = age, y = fev, color = as.factor(smoking))) +\n","  geom_point(alpha = 0.5) +\n","  geom_smooth(method = \"lm\") +\n","  labs(color = \"Smoking\", y = \"FEV\", title = \"FEV by Age and Smoking Status\")"],"metadata":{"id":"7CQwCQe4z6t-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Diagnostics\n","\n","### Check Residuals"],"metadata":{"id":"pxoXXzt70Lkn"}},{"cell_type":"code","source":["par(mfrow = c(2, 2))\n","plot(model)"],"metadata":{"id":"zpGOa0JW0Msx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look for:\n","\n","* Normal distribution (Q-Q plot)\n","* Constant variance (Residuals vs Fitted)\n","* No influential outliers\n","\n","---\n","\n","## Summary of Key Learnings\n","\n","| Concept                  | Application                                           |\n","| ------------------------ | ----------------------------------------------------- |\n","| Continuous + Categorical | Modeled together using `lm()`                         |\n","| Interaction term         | `smoke * age` checks combined effects                 |\n","| Interpretation           | Coefficients explain how each variable influences FEV |\n","| Visualization            | Helps spot trends and interaction patterns            |\n","\n","---\n","\n","## Reporting Example\n","\n","> A linear regression was conducted to predict FEV using smoking status, age, and height.\n","> Smoking had a significant negative effect on FEV (β = -0.23, p = 0.023).\n","> Age (β = 0.17, p < 0.001) and height (β = 0.09, p < 0.001) were strong positive predictors.\n","> There was no significant interaction between smoking and age (p = 0.08).\n","\n","---\n","\n","## Recap of **Regression and Analysis of variance (ANOVA)**\n","\n","You've now learned:\n","\n","* How to run and interpret **One-Way and Two-Way ANOVA**\n","* How to test for **interactions**\n","* How to combine **categorical and quantitative predictors**\n","* How to apply it to **real data (FEV)**"],"metadata":{"id":"cxtLSys1xfZS"}},{"cell_type":"markdown","source":["->"],"metadata":{"id":"cIhWHsWu29Ws"}},{"cell_type":"markdown","source":["# **7.4 Principal Component Analysis (PCA)**\n","##  **PCA (Principal Component Analysis)**\n","\n","###  Objective\n","\n","**Principal Component Analysis (PCA)** is a **dimensionality reduction technique** used to:\n","\n","* Reduce the number of variables while preserving as much variance as possible.\n","* Identify **underlying structure** in high-dimensional data.\n","* Create **uncorrelated components** from correlated predictors.\n","\n","---\n","\n","###  When to Use PCA\n","\n","* When your data has **many correlated variables**\n","* For **visualizing** high-dimensional data\n","* As a **preprocessing step** before regression, clustering, or classification\n","\n","---\n","\n","###  Key Concepts\n","\n","| Term                 | Meaning                                                                 |\n","| -------------------- | ----------------------------------------------------------------------- |\n","| Principal Components | Linear combinations of original variables that capture maximum variance |\n","| PC1, PC2, ...        | First component captures most variance, second captures remaining, etc. |\n","| Scree Plot           | A plot of variance explained by each component                          |\n","| Loadings             | Coefficients for how much each original variable contributes to a PC    |\n","\n","---\n","\n","###  Example: PCA on the `USArrests` Dataset\n","\n","The `USArrests` dataset contains crime statistics (murder, assault, urban population, rape) for U.S. states."],"metadata":{"id":"mLPfaEpGzLRv"}},{"cell_type":"code","source":["# Load the dataset\n","data(\"USArrests\")\n","head(USArrests)"],"metadata":{"id":"TfEGESa1zMGr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 1: Scale the Data\n","\n","PCA requires **scaling** because variables are on different units."],"metadata":{"id":"K_J0iHj5zgwK"}},{"cell_type":"code","source":["scaled_data <- scale(USArrests)\n","scaled_data"],"metadata":{"id":"PqZIDERczmc6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Run PCA"],"metadata":{"id":"anV0AC5Hzz8Z"}},{"cell_type":"code","source":["pca_result <- prcomp(scaled_data)\n","summary(pca_result)"],"metadata":{"id":"NhZLBhCuz04S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Output Explanation\n","\n","* **Standard deviation**: Square root of variance explained by each PC\n","* **Proportion of Variance**: How much variance each PC explains\n","* **Cumulative Proportion**: Total variance explained up to that PC\n","\n","### Step 3: Scree Plot (Variance Explained)\n"],"metadata":{"id":"PV7ekHayz-Mr"}},{"cell_type":"code","source":["plot(pca_result, type = \"l\", main = \"Scree Plot\")"],"metadata":{"id":"KLpPhjc0z-3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Biplot (Visualization)"],"metadata":{"id":"jXfYKtje0O4H"}},{"cell_type":"code","source":["biplot(pca_result, scale = 0)"],"metadata":{"id":"uW-GzMzl0PqV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Points = states\n","* Arrows = contribution of each variable to PCs\n","\n","---\n","\n","### Step 5: Get Loadings (Component Weights)"],"metadata":{"id":"ijVOAO6c0YrN"}},{"cell_type":"code","source":["pca_result$rotation"],"metadata":{"id":"PWRLhIkf0eCD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each row shows how original variables contribute to each principal component.\n","\n","---\n","\n","###  Interpretation\n","\n","* **PC1** might represent overall crime level\n","* **PC2** might contrast urban population vs. violence"],"metadata":{"id":"YMQa5XvP0k3n"}}]}